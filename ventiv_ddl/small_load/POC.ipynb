{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6777d9d6-e9f0-462d-954a-aaf157896a96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"catalog\", \"hive_metastore\")\n",
    "dbutils.widgets.text(\"schema\", \"delta_nip\")\n",
    "dbutils.widgets.text(\"date\", \"04052025\")\n",
    "dbutils.widgets.text(\"root_location\", \"/user/hive/warehouse/delta_nip.db/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2568d7be-7482-47c3-acdd-9e029c7f1634",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "\n",
    "# ls -l /Workspace/Users/jn@nipgroup.com/ventiv_ddl/small_load\n",
    "ls -l /dbfs/mnt/\n",
    "ls -l /dbfs/mnt/load\n",
    "ls -l /dbfs/mnt/schema\n",
    "ls -l /dbfs/mnt/sas\n",
    "ls -l /dbfs/mnt/pgp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2517680c-acc4-4b40-85fe-55a27973efff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "directory = \"/Workspace/Users/jn@nipgroup.com/ventiv_ddl/small_load/\"\n",
    "mnt_load = \"/dbfs/mnt/load\"\n",
    "# List files in the remote directory\n",
    "files = os.listdir(directory)\n",
    "print(files)\n",
    "\n",
    "for file in files:\n",
    "    if file.endswith(\".csv\"):\n",
    "        destination_path = os.path.join(mnt_load, file)\n",
    "        source_path = os.path.join(directory, file) # Remote file path\n",
    "\n",
    "        # Download the file to DBFS temp directory with the same name as the remote file\n",
    "        with open(source_path, \"rb\") as source_file, open(destination_path, \"wb\") as destination_file:\n",
    "            while True:\n",
    "                data = source_file.read()\n",
    "                if not data:\n",
    "                    break\n",
    "                destination_file.write(data)\n",
    "                \n",
    "            print(f\"File {file} downloaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2c50c9bb-a81c-4d87-ba2c-2ef6dd140d25",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "JUST VERIFYING DATA COLUMNS"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"/mnt/load/ref_class_code_exported_data_04052025.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Step 2: Extract the first row (original header) as a DataFrame\n",
    "header_row = df.limit(1)\n",
    "header_row.show()\n",
    "\n",
    "# # Step 3: Get the remaining rows\n",
    "# data_rows = df.subtract(header_row)\n",
    "\n",
    "# # Step 4: Combine the header row as the second row in the DataFrame\n",
    "# combined_df = data_rows.union(header_row)\n",
    "\n",
    "# # Step 5: Optionally assign default column names\n",
    "# df_import_01 = combined_df.toDF(*[f\"_c{i}\" for i in range(len(combined_df.columns))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6b53c5a5-76b0-43a5-bdb4-f3ec9688c7ec",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "GET EXISTING TABLE SCHEMA"
    }
   },
   "outputs": [],
   "source": [
    "existing_schema = spark.read.format(\"delta\").load(\"/user/hive/warehouse/delta_nip.db/ref_class_code\").schema\n",
    "existing_schema.simpleString()\n",
    "\n",
    "print(existing_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3de2d325-f321-44ac-a65f-88983b72c776",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "existing_schema = spark.read.format(\"delta\").load(\"/user/hive/warehouse/delta_nip.db/claims\").schema\n",
    "\n",
    "print(existing_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "bf7fb69a-53cb-4aa6-8cdc-b31a303855bd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "COMBINING CELL 5 & 4 CODES"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Get the schema of the delta table\n",
    "table_schema = spark.read.format(\"delta\").load(\"/user/hive/warehouse/delta_nip.db/ref_class_code\").schema\n",
    "\n",
    "print(existing_schema)\n",
    "\n",
    "# Step 2: Load \n",
    "df = spark.read.csv(\"/mnt/load/ref_class_code_exported_data_04052025.csv\", header=False, inferSchema=True)\n",
    "\n",
    "# \n",
    "df.show()\n",
    "\n",
    "# # Step 2: Extract the first row (original header) as a DataFrame\n",
    "# header_row = df.limit(1)\n",
    "# header_row.show()\n",
    "\n",
    "# # Step 3: Get the remaining rows\n",
    "# data_rows = df.subtract(header_row)\n",
    "\n",
    "# # Step 4: Combine the header row as the second row in the DataFrame\n",
    "# combined_df = data_rows.union(header_row)\n",
    "\n",
    "# # Step 5: Optionally assign default column names\n",
    "# df_import_01 = combined_df.toDF(*[f\"_c{i}\" for i in range(len(combined_df.columns))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b853144e-f578-4e0b-935f-61cf09934b78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "root = dbutils.widgets.get(\"root_location\")\n",
    "table_name = \"ref_class_code\"\n",
    "print(f\"{root}{table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "906c9923-9d25-4151-b719-8666794a0419",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Scenario: No column header"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "root = dbutils.widgets.get(\"root_location\")\n",
    "table_name = \"ref_class_code\"\n",
    "table_full_path = f\"{root}{table_name}\"\n",
    "\n",
    "# Step 1; Read schema from existing Delta table\n",
    "existing_table_schema = spark.read.format(\"delta\").load(table_full_path).schema\n",
    "# print(existing_table)\n",
    "column_names = existing_table_schema.fieldNames()\n",
    "\n",
    "# print(column_names)\n",
    "\n",
    "# Step 2: Load\n",
    "new = spark.read.csv(\"/mnt/load/ref_class_code_no_header.csv\", header=False)\n",
    "\n",
    "#\n",
    "# print(\"initial load with schema\")\n",
    "new.schema\n",
    "# df.display()\n",
    "\n",
    "######## APPROACH 2\n",
    "for i, name in enumerate(column_names):\n",
    "    df = df.withColumnRenamed(f\"_c{i}\", name)\n",
    "\n",
    "# print(\"after rename\")\n",
    "# df.schema\n",
    "# df.display()\n",
    "\n",
    "######## APPROACH 1\n",
    "\n",
    "# # Step 3: Extract the first row (original header) as a DataFrame\n",
    "# header_row = df.limit(1)\n",
    "\n",
    "# # Step 4: Get the remaining rows\n",
    "# data_rows = df.subtract(header_row)\n",
    "\n",
    "# # Step 4: Combine the header row as the second row in the DataFrame\n",
    "# combined_df = data_rows.union(header_row)\n",
    "\n",
    "# # Step 5: Optionally assign default column names\n",
    "# df_import_01 = combined_df.toDF(*[f\"_c{i}\" for i in range(len(combined_df.columns))])\n",
    "# df_import_01.display()\n",
    "# # Apply to new data without headers\n",
    "# input_df = spark.read.format(\"csv\") \\\n",
    "#   .option(\"header\", \"false\") \\\n",
    "#   .load(\"/path/to/input.csv\")\n",
    "\n",
    "# # Rename the default columns to match your schema\n",
    "# for i, name in enumerate(column_names):\n",
    "#     input_df = input_df.withColumnRenamed(f\"_c{i}\", name)\n",
    "\n",
    "# # Apply data types from schema\n",
    "# for field in existing_schema.fields:\n",
    "#     input_df = input_df.withColumn(field.name, \n",
    "#                                    input_df[field.name].cast(field.dataType))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c5bd9bbc-a63d-4260-ab96-511759a76889",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Scenario: No column header (SP PREMIUM TRIANGLES)"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "root = dbutils.widgets.get(\"root_location\")\n",
    "table_name = \"sp_premium_triangles\"\n",
    "table_full_path = f\"{root}{table_name}\"\n",
    "\n",
    "# Step 1; Read schema from existing Delta table\n",
    "existing_table_schema = spark.read.format(\"delta\").load(table_full_path).schema\n",
    "# print(existing_table)\n",
    "print(existing_table_schema)\n",
    "column_names = existing_table_schema.fieldNames()\n",
    "\n",
    "# print(column_names)\n",
    "\n",
    "# Step 2: Load\n",
    "triangle = spark.read.csv(\"/mnt/load/sp_premium_triangles_exported_data_04052025.csv\", header=False)\n",
    "\n",
    "#\n",
    "# print(\"initial load with schema\")\n",
    "triangle.schema\n",
    "# df.display()\n",
    "\n",
    "triangle_withSchema = spark.read.csv(\"/mnt/load/sp_premium_triangles_exported_data_04052025.csv\", header=False, schema=existing_table_schema)\n",
    "\n",
    "#\n",
    "# print(\"initial load with schema\")\n",
    "triangle_withSchema.schema\n",
    "# df.display()\n",
    "\n",
    "# ######## APPROACH 2\n",
    "# for i, name in enumerate(column_names):\n",
    "#     df = df.withColumnRenamed(f\"_c{i}\", name)\n",
    "\n",
    "# print(\"after rename\")\n",
    "# df.schema\n",
    "# df.display()\n",
    "\n",
    "######## APPROACH 1\n",
    "\n",
    "# # Step 3: Extract the first row (original header) as a DataFrame\n",
    "# header_row = df.limit(1)\n",
    "\n",
    "# # Step 4: Get the remaining rows\n",
    "# data_rows = df.subtract(header_row)\n",
    "\n",
    "# # Step 4: Combine the header row as the second row in the DataFrame\n",
    "# combined_df = data_rows.union(header_row)\n",
    "\n",
    "# # Step 5: Optionally assign default column names\n",
    "# df_import_01 = combined_df.toDF(*[f\"_c{i}\" for i in range(len(combined_df.columns))])\n",
    "# df_import_01.display()\n",
    "# # Apply to new data without headers\n",
    "# input_df = spark.read.format(\"csv\") \\\n",
    "#   .option(\"header\", \"false\") \\\n",
    "#   .load(\"/path/to/input.csv\")\n",
    "\n",
    "# # Rename the default columns to match your schema\n",
    "# for i, name in enumerate(column_names):\n",
    "#     input_df = input_df.withColumnRenamed(f\"_c{i}\", name)\n",
    "\n",
    "# # Apply data types from schema\n",
    "# for field in existing_schema.fields:\n",
    "#     input_df = input_df.withColumn(field.name, \n",
    "#                                    input_df[field.name].cast(field.dataType))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0ffd3357-a5b2-42b0-958a-1e6b83d6287b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "[FINAL APPROACH???] Scenario: No column header (SP PREMIUM TRIANGLES)"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "root = dbutils.widgets.get(\"root_location\")\n",
    "table_name = \"sp_premium_triangles\"\n",
    "table_full_path = f\"{root}{table_name}\"\n",
    "\n",
    "# Step 1; Read schema from existing Delta table\n",
    "existing_table_schema = spark.read.format(\"delta\").load(table_full_path).schema\n",
    "# print(existing_table_schema)\n",
    "column_names = existing_table_schema.fieldNames()\n",
    "\n",
    "# print(column_names)\n",
    "\n",
    "# Step 2: Load\n",
    "df = spark.read.csv(\"/mnt/load/sp_premium_triangles_exported_data_04052025.csv\", header=False)\n",
    "\n",
    "# df.display()\n",
    "count = len(df.schema.fieldNames())\n",
    "print(f\"no schema: {count}\")\n",
    "\n",
    "print(f\"with schema: {len(existing_table_schema.fieldNames())}\")\n",
    "\n",
    "\n",
    "# Validation\n",
    "if len(column_names) == len(df.schema.fieldNames()):\n",
    "    print(\"Schema match\")\n",
    "    # Rename the default columns to match your schema\n",
    "    for i, name in enumerate(column_names):\n",
    "        df = df.withColumnRenamed(f\"_c{i}\", name)\n",
    "    \n",
    "    print(\"pre data type enforcement\")\n",
    "    print(df.schema)\n",
    "\n",
    "    # Apply data types from schema\n",
    "    for field in existing_table_schema.fields:\n",
    "        df = df.withColumn(field.name, df[field.name].cast(field.dataType))\n",
    "\n",
    "    print(\"post data type enforcement\")\n",
    "    print(df.schema)\n",
    "else:\n",
    "    print(\"Schema mismatch\")\n",
    "    \n",
    "\n",
    "# print(\"after rename\")\n",
    "# df.schema\n",
    "# df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9cc30f3-0ee2-4371-a9fe-f77099962ac2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "\n",
    "# Read the schema metadata CSV into a pandas DataFrame\n",
    "schema_path = \"/dbfs/mnt/schema/claims_import_01_schema.csv\"  # Adjust this path\n",
    "schema_pd = pd.read_csv(schema_path)\n",
    "\n",
    "# Function to convert SQL types to Spark types with precision/scale\n",
    "def sql_to_spark_type(row):\n",
    "    sql_type = row[\"DATA_TYPE\"].lower()\n",
    "    is_nullable = row[\"IS_NULLABLE\"] == \"YES\"\n",
    "    \n",
    "    # Handle different data types with their specific attributes\n",
    "    if sql_type in [\"decimal\", \"numeric\"]:\n",
    "        precision = row[\"NUMERIC_PRECISION\"] if not pd.isna(row[\"NUMERIC_PRECISION\"]) else 38\n",
    "        scale = row[\"NUMERIC_SCALE\"] if not pd.isna(row[\"NUMERIC_SCALE\"]) else 0\n",
    "        return DecimalType(precision, scale)\n",
    "    elif sql_type in [\"varchar\", \"nvarchar\", \"char\", \"nchar\"]:\n",
    "        # If length is specified use it, otherwise use a reasonable default\n",
    "        length = row[\"CHARACTER_MAXIMUM_LENGTH\"] \n",
    "        if pd.isna(length) or length == -1:  # -1 often means MAX\n",
    "            return StringType()\n",
    "        else:\n",
    "            return StringType()  # Spark doesn't enforce string length\n",
    "    \n",
    "    # Map other types\n",
    "    type_map = {\n",
    "        \"int\": IntegerType(),\n",
    "        \"bigint\": LongType(),\n",
    "        \"smallint\": ShortType(),\n",
    "        \"tinyint\": ByteType(),\n",
    "        \"bit\": BooleanType(),\n",
    "        \"money\": DecimalType(19, 4),\n",
    "        \"float\": DoubleType(),\n",
    "        \"real\": FloatType(),\n",
    "        \"datetime\": TimestampType(),\n",
    "        \"datetime2\": TimestampType(),\n",
    "        \"date\": DateType(),\n",
    "        \"time\": TimestampType(),\n",
    "        \"text\": StringType(),\n",
    "        \"ntext\": StringType(),\n",
    "        \"binary\": BinaryType(),\n",
    "        \"varbinary\": BinaryType(),\n",
    "        \"uniqueidentifier\": StringType()\n",
    "    }\n",
    "    \n",
    "    return type_map.get(sql_type, StringType())\n",
    "\n",
    "# Build Spark schema based on SQL Server metadata\n",
    "schema_fields = []\n",
    "for _, row in schema_pd.iterrows():\n",
    "    is_nullable = row[\"IS_NULLABLE\"] == \"YES\"\n",
    "    field = StructField(\n",
    "        row[\"COLUMN_NAME\"], \n",
    "        sql_to_spark_type(row), \n",
    "        nullable=is_nullable\n",
    "    )\n",
    "    schema_fields.append(field)\n",
    "\n",
    "# Create the Spark schema\n",
    "spark_schema = StructType(schema_fields)\n",
    "# spark_schema.simpleString()\n",
    "print(spark_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca63529e-58ed-40cf-b712-dfbe7b2a3763",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "excel_file_path = \"/mnt/sas/I2I_CLAIM_NIP_20240930.CSV\"\n",
    "date_string = excel_file_path.split('/')[-1].split('_')[-1].split('.')[0]\n",
    "as_of_date = datetime.strptime(date_string, '%Y%m%d').strftime('%Y-%m-%d')\n",
    "display(as_of_date)\n",
    "\n",
    "df_import_01 = spark.read.csv(excel_file_path, header=True, inferSchema=False, schema=spark_schema)\n",
    "df_import_01.createOrReplaceTempView(\"df_import_01\")\n",
    "\n",
    "delta_ref_org = spark.read.format(\"delta\").load(\"/user/hive/warehouse/delta_nip.db/ref_organization\")\n",
    "delta_ref_org.createOrReplaceTempView(\"delta_ref_org\")\n",
    "\n",
    "delta_ref_lob = spark.read.format(\"delta\").load(\"/user/hive/warehouse/delta_nip.db/ref_lob\")\n",
    "delta_ref_lob.createOrReplaceTempView(\"delta_ref_lob\")\n",
    "\n",
    "delta_ref_cov_desc = spark.read.format(\"delta\").load(\"/user/hive/warehouse/delta_nip.db/ref_cov_desc\")\n",
    "delta_ref_cov_desc.createOrReplaceTempView(\"delta_ref_cov_desc\")                                               \n",
    "\n",
    "# joined_sql_df = spark.sql(\"\"\"\n",
    "#     SELECT \n",
    "#         c.*, \n",
    "#         d.column1, \n",
    "#         d.column2\n",
    "#     FROM \n",
    "#         csv_data c\n",
    "#     JOIN \n",
    "#         delta_data d\n",
    "#     ON \n",
    "#         c.join_column = d.join_column\n",
    "#     WHERE \n",
    "#         c.some_condition = 'value'\n",
    "# \"\"\")\n",
    "\n",
    "df_import_02 = spark.sql(f\"\"\"\n",
    "    SELECT CAST('{as_of_date}' AS DATE) AS as_of,\n",
    "        b.book,\n",
    "        b.account_name,\n",
    "        b.specialty_program,\n",
    "        b.carrier,\t\n",
    "        c.lob,\n",
    "        a.*,\n",
    "        CAST(SUBSTRING(a.claim_last_updated_date_char, 1, 9) AS DATE) AS claim_last_updated_date,\n",
    "        d.cov_desc,\n",
    "        d.cov_desc_detail\n",
    "    FROM df_import_01 AS a\n",
    "        LEFT JOIN delta_ref_org AS b ON\n",
    "            a.organization = b.organization\n",
    "        LEFT JOIN delta_ref_lob AS c ON\n",
    "            a.coverage = c.coverage\n",
    "        LEFT JOIN delta_ref_cov_desc AS d ON\n",
    "            b.book = d.book AND\n",
    "            c.lob = d.lob AND\n",
    "            a.tpa = d.tpa AND\n",
    "            a.descriptors_source_coverage = d.descriptors_source_coverage;\n",
    "\"\"\")\n",
    "\n",
    "df_import_02.display()\n",
    "\n",
    "# df_import_02_schema = spark.createDataFrame(df_import_02.rdd, schema=existing_table_schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc3667ff-7329-44a2-950f-27e4209d5ae7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import calendar\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# Parameter for how many months to go back\n",
    "time_machine = 1  # Example: go back 9 months\n",
    "\n",
    "# Get current date\n",
    "current_date = datetime.now()\n",
    "\n",
    "# Calculate target date by subtracting months (handles year transitions properly)\n",
    "target_date = current_date - relativedelta(months=time_machine)\n",
    "\n",
    "# Extract month and year from target date\n",
    "target_month = target_date.month\n",
    "target_year = target_date.year\n",
    "month_name = calendar.month_name[target_month]\n",
    "print(month_name)  # Using print instead of display for more universal compatibility\n",
    "\n",
    "# Get the last day of the target month\n",
    "last_day = calendar.monthrange(target_year, target_month)[1]\n",
    "\n",
    "# Format the date strings\n",
    "date_suffix = f\"{target_year:04d}{target_month:02d}{last_day:02d}\"\n",
    "as_of = datetime.strptime(date_suffix, '%Y%m%d').strftime('%Y-%m-%d')\n",
    "\n",
    "print(f\"Date suffix: {date_suffix}\")\n",
    "print(f\"As of date: {as_of}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0fbf0698-1190-4b84-8e5a-dae783284f23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "map = {\n",
    "    \"claims\": f\"I2I_CLAIM_NIP_{date_suffix}.CSV\",\n",
    "    \"exposures\": f\"I2I_EXPOSURE_NIP_{date_suffix}.CSV\",\n",
    "    \"organization\": f\"I2I_ORGANIZATION_NIP_{date_suffix}.CSV\",\n",
    "    \"policy\": f\"I2I_POLICY_NIP_{date_suffix}.CSV\",\n",
    "    \"premium\": f\"I2I_PREMIUM_NIP_{date_suffix}.CSV\",\n",
    "    \"claims_jif\": f\"I2I_JIF_CLAIM_NIP_{date_suffix}.CSV\",\n",
    "    \"exposures_jif\": f\"I2I_JIF_EXPOSURE_NIP_{date_suffix}.CSV\",\n",
    "    \"organization_jif\": f\"I2I_JIF_ORGANIZATION_NIP_{date_suffix}.CSV\",\n",
    "    \"policy_jif\": f\"I2I_JIF_POLICY_NIP_{date_suffix}.CSV\",\n",
    "    \"premium_jif\": f\"I2I_JIF_PREMIUM_NIP_{date_suffix}.CSV\",\n",
    "    \"claims_prog\": f\"I2I_Programs_CLAIM_NIP_{date_suffix}.CSV\",\n",
    "    \"exposures_prog\": f\"I2I_Programs_EXPOSURE_NIP_{date_suffix}.CSV\",\n",
    "    \"organization_prog\": f\"I2I_Programs_ORGANIZATION_NIP_{date_suffix}.CSV\",\n",
    "    \"policy_prog\": f\"I2I_Programs_POLICY_NIP_{date_suffix}.CSV\",\n",
    "    \"premium_prog\": f\"I2I_Programs_PREMIUM_NIP_{date_suffix}.CSV\"\n",
    "}\n",
    "\n",
    "table_name = \"claims\"\n",
    "file_name = map.get(table_name)\n",
    "\n",
    "files = dbutils.fs.ls(\"/mnt/sas\")\n",
    "\n",
    "print(\"default\")\n",
    "if any(file_info.name == file_name for file_info in files):\n",
    "    # return the full file path if the file exists\n",
    "    print(f\"/mnt/sas/{file_name}\") # Use print for demo only\n",
    "else:\n",
    "    print(\"jif\")\n",
    "    file_name = map.get(f\"{table_name}_jif\")\n",
    "    files = dbutils.fs.ls(\"/mnt/sas\")\n",
    "\n",
    "    if any(file_info.name == file_name for file_info in files):\n",
    "        # return the full file path if the file exists\n",
    "        print(f\"/mnt/sas/{file_name}\") # Use print for demo only\n",
    "    else:\n",
    "        print(\"prog\")\n",
    "        file_name = map.get(f\"{table_name}_prog\")\n",
    "        files = dbutils.fs.ls(\"/mnt/sas\")\n",
    "\n",
    "        if any(file_info.name == file_name for file_info in files):\n",
    "            # return the full file path if the file exists\n",
    "            print(f\"/mnt/sas/{file_name}\") # Use print for demo only\n",
    "        else:\n",
    "            # Last iteration, return None if the file does not exist\n",
    "            print(\"File not found\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6806813852384970,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "POC",
   "widgets": {
    "catalog": {
     "currentValue": "hive_metastore",
     "nuid": "797c3ad7-365d-4ab5-a44d-b87a752ad3c5",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "hive_metastore",
      "label": null,
      "name": "catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "hive_metastore",
      "label": null,
      "name": "catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "date": {
     "currentValue": "04052025",
     "nuid": "cc4d61c7-a431-4a21-a1a4-ca9554ad2405",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "04052025",
      "label": null,
      "name": "date",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "04052025",
      "label": null,
      "name": "date",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "root_location": {
     "currentValue": "/user/hive/warehouse/delta_nip.db/",
     "nuid": "ef58c1a8-7dad-47a0-a482-7265fa5f98a9",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "/user/hive/warehouse/delta_nip.db/",
      "label": null,
      "name": "root_location",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "/user/hive/warehouse/delta_nip.db/",
      "label": null,
      "name": "root_location",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "schema": {
     "currentValue": "delta_nip",
     "nuid": "60f4c48d-b9ad-41e6-95f6-61e4a8f9f150",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "delta_nip",
      "label": null,
      "name": "schema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "delta_nip",
      "label": null,
      "name": "schema",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
