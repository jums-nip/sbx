{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c786f20f-5178-47ce-bea0-abc7a3b651a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, substring, to_date, lit\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "import pandas as pd\n",
    "\n",
    "# Function to load reference tables as temporary views\n",
    "def load_reference_tables(db_path: str = \"/user/hive/warehouse/delta_nip.db/\"):\n",
    "    \"\"\"\n",
    "    Loads reference Delta tables and registers them as temporary views.\n",
    "\n",
    "    Parameters:\n",
    "    - db_path: Base path to the delta database directory\n",
    "\n",
    "    Outputs:\n",
    "    - None, but creates temporary views for each table\n",
    "    \"\"\"\n",
    "\n",
    "    tables = {\n",
    "        \"ref_organization\": \"delta_ref_org\",\n",
    "        \"ref_lob\": \"delta_ref_lob\",\n",
    "        \"ref_cov_desc\": \"delta_ref_cov_desc\"\n",
    "    }\n",
    "\n",
    "    for table_name, view_name in tables.items():\n",
    "        df = spark.read.format(\"delta\").load(f\"{db_path}{table_name}\")\n",
    "        df.createOrReplaceTempView(view_name)\n",
    "        print(f\"View '{view_name}' created from '{table_name}'\")\n",
    "\n",
    "# Function to get and format the as_of value and the target file suffixed by the fommatted date\n",
    "def calculate_as_of_date(time_machine=1):\n",
    "    \"\"\"\n",
    "    Calculate the 'as of' date by going back a specified number of months from the current date.\n",
    "\n",
    "    Parameters:\n",
    "    - time_machine (int): Number of months to go back from the current date. Default is 1.\n",
    "\n",
    "    Returns:\n",
    "    - str: The 'date suffix' in the format 'YYYYMMDD' and 'as of' date in 'YYYY-MM-DD' format.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get current date\n",
    "    current_date = datetime.now()\n",
    "\n",
    "    # Calculate target date by subtracting months (handles year transitions properly)\n",
    "    target_date = current_date - relativedelta(months=time_machine)\n",
    "\n",
    "    # Extract month and year from target date\n",
    "    target_month = target_date.month\n",
    "    target_year = target_date.year\n",
    "\n",
    "    # Get the last day of the target month\n",
    "    last_day = calendar.monthrange(target_year, target_month)[1]\n",
    "\n",
    "    # Format the date strings\n",
    "    date_suffix = f\"{target_year:04d}{target_month:02d}{last_day:02d}\"\n",
    "    as_of = datetime.strptime(date_suffix, '%Y%m%d').strftime('%Y-%m-%d')\n",
    "\n",
    "    return date_suffix, as_of\n",
    "\n",
    "# Function to get the file path based on the table name, date suffix, and book type\n",
    "def get_file_path(table_name, date_suffix, book=\"default\"):\n",
    "    \"\"\"\n",
    "    Get the file path based on the table name, date suffix, and book type.\n",
    "\n",
    "    Parameters:\n",
    "    - table_name (str): The name of the table.\n",
    "    - date_suffix (str): The date suffix in the format 'YYYYMMDD'.\n",
    "    - book (str): The book type (\"jif\", \"program\", or \"default\"). Default is \"default\".\n",
    "\n",
    "    Returns:\n",
    "    - str: The file path to the file.\n",
    "    \"\"\"\n",
    "\n",
    "    file_map = {\n",
    "        # Files prior to split between JIF and Programs\n",
    "        \"default\": {\n",
    "            \"claims\": f\"I2I_CLAIM_NIP_{date_suffix}.CSV\",\n",
    "            \"exposures\": f\"I2I_EXPOSURE_NIP_{date_suffix}.CSV\",\n",
    "            \"organization\": f\"I2I_ORGANIZATION_NIP_{date_suffix}.CSV\",\n",
    "            \"policy\": f\"I2I_POLICY_NIP_{date_suffix}.CSV\",\n",
    "            \"premium\": f\"I2I_PREMIUM_NIP_{date_suffix}.CSV\"\n",
    "        },\n",
    "        # JIF files\n",
    "        \"jif\": {\n",
    "            \"claims\": f\"I2I_JIF_CLAIM_NIP_{date_suffix}.CSV\",\n",
    "            \"exposures\": f\"I2I_JIF_EXPOSURE_NIP_{date_suffix}.CSV\",\n",
    "            \"organization\": f\"I2I_JIF_ORGANIZATION_NIP_{date_suffix}.CSV\",\n",
    "            \"policy\": f\"I2I_JIF_POLICY_NIP_{date_suffix}.CSV\",\n",
    "            \"premium\": f\"I2I_JIF_PREMIUM_NIP_{date_suffix}.CSV\"\n",
    "        },\n",
    "        # Programs files\n",
    "        \"program\": {\n",
    "            \"claims\": f\"I2I_Programs_CLAIM_NIP_{date_suffix}.CSV\",\n",
    "            \"exposures\": f\"I2I_Programs_EXPOSURE_NIP_{date_suffix}.CSV\",\n",
    "            \"organization\": f\"I2I_Programs_ORGANIZATION_NIP_{date_suffix}.CSV\",\n",
    "            \"policy\": f\"I2I_Programs_POLICY_NIP_{date_suffix}.CSV\",\n",
    "            \"premium\": f\"I2I_Programs_PREMIUM_NIP_{date_suffix}.CSV\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    book = book if book in [\"jif\", \"program\"] else \"default\"\n",
    "    file_name = file_map[book].get(table_name)\n",
    "\n",
    "    files = dbutils.fs.ls(\"/mnt/sas\")\n",
    "\n",
    "    if any(file_info.name == file_name for file_info in files):\n",
    "        return f\"/mnt/sas/{file_name}\"\n",
    "    \n",
    "    return \"File not found\"\n",
    "\n",
    "# Function to convert SQL types to Spark types with precision/scale\n",
    "def sql_to_spark_type(row):\n",
    "    \"\"\"\n",
    "    Converts SQL types to Spark types with precision/scale.\n",
    "\n",
    "    Parameters:\n",
    "    - row: A row from the SQL Server metadata DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - A Spark type object.\n",
    "    \"\"\"\n",
    "\n",
    "    sql_type = row[\"DATA_TYPE\"].lower()\n",
    "    is_nullable = row[\"IS_NULLABLE\"] == \"YES\"\n",
    "    \n",
    "    # Handle different data types with their specific attributes \n",
    "    if sql_type in [\"nvarchar\", \"char\"]:\n",
    "        # If length is specified use it, otherwise use a reasonable default\n",
    "        length = row[\"CHARACTER_MAXIMUM_LENGTH\"] \n",
    "        if pd.isna(length) or length == -1:  # -1 often means MAX\n",
    "            return StringType()\n",
    "        else:\n",
    "            return StringType()  # Spark doesn't enforce string length\n",
    "    \n",
    "    # Map other types\n",
    "    type_map = {\n",
    "        \"float\": FloatType(),\n",
    "        \"date\": DateType()\n",
    "    }\n",
    "    \n",
    "    return type_map.get(sql_type, StringType())\n",
    "\n",
    "# Function that builds Spark schema based on SQL Server metadata\n",
    "def build_spark_schema(schema_pd):\n",
    "    \"\"\"\n",
    "    Builds a Spark schema based on the SQL Server metadata.\n",
    "\n",
    "    Parameters:\n",
    "    - schema_pd: A pandas DataFrame containing the SQL Server metadata.\n",
    "\n",
    "    Returns:\n",
    "    - A list of StructField objects representing the Spark schema.\n",
    "    \"\"\"\n",
    "\n",
    "    schema_fields = []\n",
    "    for _, row in schema_pd.iterrows():\n",
    "        is_nullable = row[\"IS_NULLABLE\"] == \"YES\"\n",
    "        field = StructField(\n",
    "            row[\"COLUMN_NAME\"], \n",
    "            sql_to_spark_type(row), \n",
    "            nullable=is_nullable\n",
    "        )\n",
    "        schema_fields.append(field)\n",
    "    return schema_fields\n",
    "\n",
    "# Function to align the schema of the import_01 DataFrame to the target import_02 delta table\n",
    "def align_schema(df, target_df):\n",
    "    \"\"\" Aligns the schema of the input DataFrame to match the target DataFrame's schema.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The input DataFrame to align.\n",
    "    - target_df (DataFrame): The target DataFrame whose schema to match.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: A new DataFrame with the aligned schema.\n",
    "    \"\"\"\n",
    "    # Get column lists for both DataFrames\n",
    "    target_columns = set(target_df.columns)\n",
    "    joined_columns = set(df.columns)\n",
    "\n",
    "    # Build select expressions that match the target schema\n",
    "    select_expressions = []\n",
    "\n",
    "    for field in target_df.schema.fields:\n",
    "        field_name = field.name\n",
    "\n",
    "        if field_name in joined_columns:\n",
    "            # Column exists, cast it to match the target schema\n",
    "            select_expressions.append(\n",
    "                col(field_name).cast(field.dataType).alias(field_name)\n",
    "            )\n",
    "        else:\n",
    "            # Column doesn't exist, add a null with correct type\n",
    "            select_expressions.append(\n",
    "                lit(None).cast(field.dataType).alias(field_name)\n",
    "            )\n",
    "\n",
    "    # Apply the select expressions to create a DataFrame with matching schema\n",
    "    return df.select(*select_expressions)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "init_func",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
